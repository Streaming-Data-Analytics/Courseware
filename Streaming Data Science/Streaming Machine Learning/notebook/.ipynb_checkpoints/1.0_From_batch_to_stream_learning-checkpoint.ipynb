{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From batch to stream learning\n",
    "---\n",
    "\n",
    "Traditional machine learning follows a sequence of steps. A minimal pipeline includes:\n",
    "\n",
    "1. Data preprocessing (cleanup, filtering, feature engineering, etc.)\n",
    "2. Training (create a model from the data)\n",
    "3. Testing (use the model to obtain predictions)\n",
    "\n",
    "This approach assumes that **all** the data is available for training and data is processed in **batches**.\n",
    "\n",
    "Let's see an example based on the [`Iris` dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) available in `scikit-learn`:\n",
    "\n",
    "> This data set consists of 3 different types of irisesâ€™ (Setosa, Versicolour, and Virginica) petal and sepal width and length, stored in a 150x4 numpy.ndarray\n",
    "\n",
    "![iris example](../img/iris_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets as skdatasets\n",
    "from sklearn.naive_bayes import GaussianNB as GaussianNBSKL   # Renamed to avoid name conflicts\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('classic')\n",
    "sns.set(style=\"ticks\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = skdatasets.load_iris()\n",
    "x = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0],y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(sns.load_dataset(\"iris\"), hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_iris()` returns the data set and related information. Data features and the corresponding targets (classification labels) are accessible via the `data` and `target` properties respectively.\n",
    "\n",
    "We can assume that the data has been already preprocessed. We need to split the dataset into $train$ and $test$ sets where $train \\notin test$. We can use the `train_test_split` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.3, stratify=iris.target, random_state=42\n",
    ")\n",
    "print(f'Shape (n_rows, n_cols) of X: {iris.data.shape}, 'f'X_train: {X_train.shape}, X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed with the model training. In this example, we will train a Gaussian Naive Bayes model (`GaussianNB`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNBSKL()\n",
    "model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we use the trained model to get predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_pred` contains the predicted class labels for each sample in `X_test`. Since we know the true labels for these samples (`y_test`) we can evaluate the performance of the model. In this case, we will use the `accuracy_score` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model accuracy: {accuracy_score(y_test, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model has an accuracy of ~91%, which is decent considering the simplicity of the dataset.\n",
    "\n",
    "As mentioned, batch methods expect **all the data** available for training. However, in multiple real-world applications, this requirement is difficult to meet, especially in dynamic environments that constantly generate data.\n",
    "\n",
    "A common way to address this situation is to collect data over time and generate new models. However, this presents additional challenges:\n",
    "\n",
    "- Should we train new models on new data only or extend the old data?\n",
    "- How much new data is enough to trigger the creation of a new model?\n",
    "- Is the new model better than the old one?\n",
    "- Storing all data might be unfeasible or impractical in some cases (e.g. financial markets)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream learning\n",
    "\n",
    "A data stream is an infinite sequence of elements. Stream learning methods are specially designed to learn from data streams. Streaming models adhere to a set of requirements:\n",
    "\n",
    "- **Process the data one sample at a time**. A sample is seen only once and it is gone after processing.\n",
    "- **Be efficient**. Resources are limited (memory, time) but streams are infinite.\n",
    "- **Predict at any time**. Models continually learn and provide predictions at any point in the stream.\n",
    "\n",
    "A streaming model **updates** its internal state as it processes the data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simulate a data stream from the previous example by \"looping\" over the data *one sample at a time* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xi, yi in zip(X_train, y_train):\n",
    "    # Here is where learning takes place\n",
    "    pass\n",
    "\n",
    "xi, yi    # last data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the data is encoded in `numpy.ndarrays`, an efficient data structure when dealing with large 2D data.\n",
    "\n",
    "In `river`, data is represented as `dict`. Dictionaries are fast to process 1D data and elements are easily accessible 'by key'.\n",
    "\n",
    "Before using this data in `river` we must transform it. This is easily done in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xi, yi in zip(X_train, y_train):\n",
    "    x = dict(zip(iris.feature_names, xi))\n",
    "\n",
    "x, yi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![River-logo](../img/river_logo.png)\n",
    "\n",
    "[`River`](https://riverml.xyz/0.10.1/) is a Python library for streaming (online) machine learning.\n",
    "\n",
    "\n",
    "### Key features\n",
    "\n",
    "- **Incremental learning** - All the tools in `river` can be updated with a single sample at a time.\n",
    "\n",
    "- **Adaptive learning** - Adaptive methods are robust against concept drift in dynamic environments.\n",
    "\n",
    "- **General-purpose** - `River` caters for different machine learning problems, including regression, classification, unsupervised learning, and ad-hoc tasks.\n",
    "\n",
    "- **Efficient** - By design, streaming techniques efficiently handle resources such as memory and processing time, given the unbounded nature of data streams.\n",
    "\n",
    "- **Easy to use** - `River` is intended for users with any experience level. As a machine learning package, it caters for practitioners as well as researchers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous example, we trained a Gaussian Naive Bayes model using the implementation from `scikit-learn`. The Naive Bayes method is an **incremental method**.\n",
    "\n",
    "A small number of [incremental methods](https://scikit-learn.org/stable/modules/computing.html#scaling-with-instances-using-out-of-core-learning) are available in `scikit-learn`. These methods provide a `partial_fit` method for incremental training using *mini-batches*.\n",
    "\n",
    "In the following example, we will use the implementation of `GaussianNB` from `river`.\n",
    "\n",
    "In `river`, training and predicting are perfomed via the `learn_one` and `predict_one` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "for xi, yi in zip(X_train, y_train):\n",
    "    x = dict(zip(iris.feature_names, xi))\n",
    "    model.learn_one(x=x, y=yi)              # Train the model\n",
    "\n",
    "y_pred = []\n",
    "for xi in X_test:\n",
    "    x = dict(zip(iris.feature_names, xi))\n",
    "    y_pred.append(model.predict_one(x=x))   # Predict class-label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's predictions are in `y_pred`. Again, we measure the accuracy of the model by comparing predictions to the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model accuracy: {accuracy_score(y_test, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray! We got the same performance from the model trained one sample at a time...\n",
    "\n",
    "But wait, this is NOT different from the \"batch\" setting. We are still doing \"training\" and then \"testing\" using two \"for\" loops.\n",
    "\n",
    "In the streaming setting, training and predicting happen inside a single loop. Remember, a data sample is available only once in the stream.\n",
    "\n",
    "To simplify our code, we can use the [iter_sklearn](https://riverml.xyz/0.10.1/api/stream/iter-sklearn-dataset/) method to convert a `scikit-learn` dataset object into an iterable that returns the data as `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.stream import iter_sklearn_dataset\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "for x, y in iter_sklearn_dataset(iris, shuffle=True, seed=42):\n",
    "    y_p = model.predict_one(x)   # Predict class-label\n",
    "    model.learn_one(x, y)        # Train the model\n",
    "    if y_p is None:              # Process next sample if the model is empty\n",
    "        continue\n",
    "    y_pred.append(y_p)\n",
    "    y_true.append(y)\n",
    "\n",
    "print(f'Model accuracy: {accuracy_score(y_true, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we have successfully implemented a stream learning task. This is known as **prequential evaluation** or **interleaved test-then-train** evaluation. \n",
    "\n",
    "Notice the *order* of the training and prediction operations. When a new data sample ($x_i$, $y_i$) arrives:\n",
    "\n",
    "1. Predict the label ($y_i'$) for the new data sample ($x_i$), not \"seen\" by the model yet\n",
    "2. Train the model using the new sample ($x_i$, $y_i$)\n",
    "\n",
    "This **order matters**!\n",
    "\n",
    "A benefit from this approach is that we can use *all* labeled samples in the stream to measure a model's performance. Notice that the model provides predictions at any point in the stream, depending on its current internal state. Batch methods can only predict after they finished training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code snippet, we used two auxiliary variables to store the true labels and the predicted labels. This is because `accuracy_score` requires access to all the data. However, storing all the true and predicted labels from a data stream is *impractical*.\n",
    "\n",
    "In `river`, all methods can be updated one single sample at a time, including performance metrics! Let's see how to update the [Accuracy](https://riverml.xyz/latest/api/metrics/Accuracy/) metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.metrics import Accuracy\n",
    "\n",
    "metric = Accuracy()\n",
    "for yt, yp in zip(y_true, y_pred):\n",
    "    metric.update(y_true=yt, y_pred=yp)\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `update` method takes true and predicted pairs and updates the metric. We can compare the confusion matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion matrix from all data:')\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print('Confusion matrix from updatable metric:')\n",
    "print(metric.cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can rewrite our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "metric = Accuracy()\n",
    "\n",
    "for x, y in iter_sklearn_dataset(iris, shuffle=True, seed=42):\n",
    "    y_p = model.predict_one(x)   # Predict class\n",
    "    if y_p is not None:\n",
    "        metric.update(y_true=y, y_pred=y_p)\n",
    "    model.learn_one(x, y)        # Train the model\n",
    "\n",
    "print(f'{len(y_pred)} samples analyzed.')   \n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `progressive_val_score` method\n",
    "\n",
    "For convenience, the [progressive_val_score](https://riverml.xyz/0.10.1/api/evaluate/progressive-val-score/) method implements the prequential evaluation. It requires three components: \n",
    "\n",
    "- a data stream\n",
    "- a model\n",
    "- a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.evaluate import progressive_val_score\n",
    "\n",
    "# Setup stream and estimators\n",
    "stream = iter_sklearn_dataset(iris, shuffle=True, seed=42)\n",
    "nb = GaussianNB()\n",
    "metric = Accuracy()\n",
    "\n",
    "# Setup evaluator\n",
    "progressive_val_score(dataset=stream, model=nb, metric=metric, print_every=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
